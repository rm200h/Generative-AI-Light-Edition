{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8af3794b",
   "metadata": {},
   "source": [
    "# Chat completion: Run Llama 2 models in SageMaker JumpStart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ea02ec",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook.\n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-west-2/introduction_to_amazon_algorithms|jumpstart-foundation-models|llama-2-chat-completion.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446b1b24",
   "metadata": {},
   "source": [
    "---\n",
    "In this demo notebook, we demonstrate how to use the SageMaker Python SDK to deploy a JumpStart model for Text Generation using the Llama 2 fine-tuned model optimized for dialogue use cases.\n",
    "\n",
    "To perform inference on these models, you need to pass custom_attributes='accept_eula=true' as part of header. This means you have read and accept the end-user-license-agreement (EULA) of the model. EULA can be found in model card description or from https://ai.meta.com/resources/models-and-libraries/llama-downloads/. By default, this notebook sets custom_attributes='accept_eula=false', so all inference requests will fail until you explicitly change this custom attribute.\n",
    "\n",
    "Note: Custom_attributes used to pass EULA are key/value pairs. The key and value are separated by '=' and pairs are separated by ';'. If the user passes the same key more than once, the last value is kept and passed to the script handler (i.e., in this case, used for conditional logic). For example, if 'accept_eula=false; accept_eula=true' is passed to the server, then 'accept_eula=true' is kept and passed to the script handler.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35642ab2",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b55e677-3429-4668-b100-bd63d2a4c401",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d458cf0-02e2-4066-927b-25fa5ef2a07e",
   "metadata": {},
   "source": [
    "***\n",
    "You can continue with the default model or choose a different model: this notebook will run with the following model IDs :\n",
    "- `meta-textgeneration-llama-2-7b-f`\n",
    "- `meta-textgeneration-llama-2-13b-f`\n",
    "- `meta-textgeneration-llama-2-70b-f`\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "014424a8-7f8f-46a7-8963-2c3d454878b8",
   "metadata": {
    "jumpStartAlterations": [
     "modelIdVersion"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    model_id,\n",
    "    model_version,\n",
    ") = (\n",
    "    \"meta-textgeneration-llama-2-7b-f\",\n",
    "    \"*\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eef0dd",
   "metadata": {},
   "source": [
    "## Deploy model\n",
    "\n",
    "***\n",
    "You can now deploy the model using SageMaker JumpStart.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e52afae-868d-4736-881f-7180f393003a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------!CPU times: user 148 ms, sys: 19 ms, total: 167 ms\n",
      "Wall time: 9min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "import time\n",
    "start = time.time()\n",
    "model = JumpStartModel(model_id=model_id)\n",
    "predictor = model.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b4d109",
   "metadata": {},
   "source": [
    "### Changing instance type\n",
    "---\n",
    "\n",
    "\n",
    "Models are supported on the following instance types:\n",
    "\n",
    " - Llama 2 7B and 7B-F: `ml.g5.2xlarge`, `ml.g5.4xlarge`, `ml.g5.8xlarge`, `ml.g5.12xlarge`, `ml.g5.24xlarge`, `ml.g5.48xlarge`, `ml.p4d.24xlarge`\n",
    " - Llama 2 13B and 13B-F: `ml.g5.12xlarge`, `ml.g5.24xlarge`, `ml.g5.48xlarge`, `ml.p4d.24xlarge`\n",
    " - Llama 2 70B and 70B-F: `ml.g5.48xlarge`, `ml.p4d.24xlarge`\n",
    "\n",
    "By default, the JumpStartModel class selects a default instance type available in your region. If you would like to use a different instance type, you can do so by specifying instance type in the JumpStartModel class.\n",
    "\n",
    "`my_model = JumpStartModel(model_id=model_id, instance_type=\"ml.g5.12xlarge\")`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef7207e-01ba-4ac2-b4a9-c8f6f0e1c498",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Invoke the endpoint\n",
    "\n",
    "***\n",
    "### Supported Parameters\n",
    "This model supports the following inference payload parameters:\n",
    "\n",
    "* **max_new_tokens:** Model generates text until the output length (excluding the input context length) reaches max_new_tokens. If specified, it must be a positive integer.\n",
    "* **temperature:** Controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words. If `temperature` -> 0, it results in greedy decoding. If specified, it must be a positive float.\n",
    "* **top_p:** In each step of text generation, sample from the smallest possible set of words with cumulative probability `top_p`. If specified, it must be a float between 0 and 1.\n",
    "\n",
    "You may specify any subset of the parameters mentioned above while invoking an endpoint. \n",
    "\n",
    "***\n",
    "### Notes\n",
    "- This model only supports 'system', 'user' and 'assistant' roles, starting with 'system', then 'user' and alternating (u/a/u/a/u...).\n",
    "- If `max_new_tokens` is not defined, the model may generate up to the maximum total tokens allowed, which is 4K for these models. This may result in endpoint query timeout errors, so it is recommended to set `max_new_tokens` when possible. For 7B, 13B, and 70B models, we recommend to set `max_new_tokens` no greater than 1500, 1000, and 500 respectively, while keeping the total number of tokens less than 4K.\n",
    "- In order to support a 4k context length, this model has restricted query payloads to only utilize a batch size of 1. Payloads with larger batch sizes will receive an endpoint error prior to inference.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c5adf9b4-c7e1-4090-aefe-9cae0d096968",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_dialog(payload, response):\n",
    "    dialog = payload[\"inputs\"][0]\n",
    "    for msg in dialog:\n",
    "        print(f\"{msg['role'].capitalize()}: {msg['content']}\\n\")\n",
    "    print(\n",
    "        f\"> {response[0]['generation']['role'].capitalize()}: {response[0]['generation']['content']}\"\n",
    "    )\n",
    "    print(\"\\n==================================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fbb9af",
   "metadata": {},
   "source": [
    "***\n",
    "### Example 1\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4cbde5e7-1068-41f9-999a-70ef04e1cbbb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: what is the recipe of mayonnaise?\n",
      "\n",
      "> Assistant:  Mayonnaise is a thick, creamy condiment made from a mixture of egg yolks, oil, vinegar or lemon juice, and seasonings. Here is a basic recipe for homemade mayonnaise:\n",
      "\n",
      "Ingredients:\n",
      "\n",
      "* 2 egg yolks\n",
      "* 1/2 cup (120 ml) neutral-tasting oil, such as canola or grapeseed\n",
      "* 1 tablespoon (15 ml) vinegar or lemon juice\n",
      "* 1/2 teaspoon salt\n",
      "* 1/4 teaspoon black pepper\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. In a small bowl, whisk together the egg yolks and salt until well combined.\n",
      "2. Slowly pour the oil into the egg yolk mixture while continuously whisking. The mixture will begin to thicken and emulsify as you add the oil.\n",
      "3. Once you have added about 1/4 cup (60 ml) of oil, stop whisking and let the mixture sit for 5-10 minutes to allow the oil and egg yolks to fully emulsify.\n",
      "4. After the mixture has sat for a few minutes, give it a few quick whisk strokes to check that it has thickened evenly. If it's still a bit too thin, let it sit for a few more minutes and try again.\n",
      "5. Add the vinegar or lemon juice and whisk until fully incorporated.\n",
      "6. Taste and adjust the seasoning as needed. You can add more salt, pepper, or vinegar to taste.\n",
      "7. Cover the bowl with plastic wrap and refrigerate the mayonnaise for at least 30 minutes before using. This will allow the flavors to meld together and the mayonnaise to thicken further.\n",
      "\n",
      "That's it! This recipe makes about 1 1/2 cups (360 ml) of mayonnaise, which you can use in sandwiches, salads, dips, and other dishes. You can also customize the recipe by adding different seasonings or flavorings, such as garlic, mustard, or hot sauce. Enjoy!\n",
      "\n",
      "==================================\n",
      "\n",
      "CPU times: user 15 ms, sys: 0 ns, total: 15 ms\n",
      "Wall time: 16.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": \"what is the recipe of mayonnaise?\"},\n",
    "        ]\n",
    "    ],\n",
    "    \"parameters\": {\"max_new_tokens\": 512, \"top_p\": 0.9, \"temperature\": 0.6},\n",
    "}\n",
    "try:\n",
    "    response = predictor.predict(payload, custom_attributes=\"accept_eula=false\")\n",
    "    print_dialog(payload, response)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5574e4e2",
   "metadata": {},
   "source": [
    "***\n",
    "### Example 2\n",
    "use of assitant role\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cda81ccf-0188-4117-8355-801ef98aaa48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: I am going to Paris, what should I see?\n",
      "\n",
      "Assistant: Paris, the capital of France, is known for its stunning architecture, art museums, historical landmarks, and romantic atmosphere. Here are some of the top attractions to see in Paris:\n",
      "\n",
      "1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.\n",
      "2. The Louvre Museum: The Louvre is one of the world's largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.\n",
      "3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.\n",
      "\n",
      "These are just a few of the many attractions that Paris has to offer. With so much to see and do, it's no wonder that Paris is one of the most popular tourist destinations in the world.\n",
      "\n",
      "User: What is so great about #1?\n",
      "\n",
      "> Assistant:  The Eiffel Tower is considered one of the most iconic and recognizable landmarks in the world, and there are several reasons why it's so great:\n",
      "\n",
      "1. Unique Design: The Eiffel Tower's design is unlike any other tower in the world. Its lattice-like structure is made up of over 18,000 pieces of iron, held together by over 2.5 million rivets. The tower's shape and design are both functional and aesthetically pleasing, making it a true engineering marvel.\n",
      "2. Historical Significance: The Eiffel Tower was built for the 1889 World's Fair in Paris, held to celebrate the 100th anniversary of the French Revolution. It was originally intended to be a temporary structure, but it became an instant icon of Paris and a symbol of French culture and engineering.\n",
      "3. Panoramic Views: The Eiffel Tower offers stunning panoramic views of Paris from its top platform. Visitors can see many of the city's famous landmarks, including the Seine River, the Arc de Triomphe, and the Champs-Ã‰lysÃ©es.\n",
      "4. Cultural Significance: The Eiffel Tower has been featured in countless films, books, and works of art, and has become a symbol of romance and adventure. It's a must-visit destination for many tourists, and is often the first thing that comes to mind when people think of Paris.\n",
      "5. Engineering Marvel: The Eiffel Tower is an engineering marvel, with a height of 324 meters (1,063 feet) and a weight of over 10,000 tons. It was built using a revolutionary technique called \"puddle technology,\" which allowed the tower to be constructed quickly and efficiently.\n",
      "6. Accessibility: The Eiffel Tower is easily accessible by public transportation, with several metro stations nearby. Visitors can also take a lift or stairs to the top platform, which offers breathtaking views of the city.\n",
      "7. Romantic Atmosphere: The Eiffel Tower is often associated with romance, thanks to its picturesque views and iconic status. Many couples come to the tower to propose, get married, or simply enjoy a romantic evening together.\n",
      "\n",
      "Overall\n",
      "\n",
      "==================================\n",
      "\n",
      "CPU times: user 4.75 ms, sys: 192 Âµs, total: 4.94 ms\n",
      "Wall time: 15.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": \"I am going to Paris, what should I see?\"},\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\"\"\\\n",
    "Paris, the capital of France, is known for its stunning architecture, art museums, historical landmarks, and romantic atmosphere. Here are some of the top attractions to see in Paris:\n",
    "\n",
    "1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.\n",
    "2. The Louvre Museum: The Louvre is one of the world's largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.\n",
    "3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.\n",
    "\n",
    "These are just a few of the many attractions that Paris has to offer. With so much to see and do, it's no wonder that Paris is one of the most popular tourist destinations in the world.\"\"\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": \"What is so great about #1?\"},\n",
    "        ]\n",
    "    ],\n",
    "    \"parameters\": {\"max_new_tokens\": 512, \"top_p\": 0.9, \"temperature\": 0.6},\n",
    "}\n",
    "try:\n",
    "    response = predictor.predict(payload, custom_attributes=\"accept_eula=false\")\n",
    "    print_dialog(payload, response)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa8d152",
   "metadata": {},
   "source": [
    "***\n",
    "### Example 3\n",
    "Use of system role\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "de6e8250-88c8-4b1c-a70b-ae5a4976e6ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: Always answer with Haiku\n",
      "\n",
      "User: I am going to Paris, what should I see?\n",
      "\n",
      "> Assistant:  Eiffel Tower high\n",
      "Love locks on bridge embrace\n",
      "City of light shines\n",
      "\n",
      "==================================\n",
      "\n",
      "CPU times: user 2.05 ms, sys: 2.45 ms, total: 4.51 ms\n",
      "Wall time: 643 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": \"Always answer with Haiku\"},\n",
    "            {\"role\": \"user\", \"content\": \"I am going to Paris, what should I see?\"},\n",
    "        ]\n",
    "    ],\n",
    "    \"parameters\": {\"max_new_tokens\": 512, \"top_p\": 0.9, \"temperature\": 0.6},\n",
    "}\n",
    "try:\n",
    "    response = predictor.predict(payload, custom_attributes=\"accept_eula=false\")\n",
    "    print_dialog(payload, response)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076644d4",
   "metadata": {},
   "source": [
    "***\n",
    "### Example 4\n",
    "Use of system role\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2da83b4a-1e61-495c-b509-38266f5c44eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: Always answer with emojis\n",
      "\n",
      "User: How to go from Beijing to NY?\n",
      "\n",
      "> Assistant:  Here's how to go from Beijing to NY ðŸ›¬ðŸ—½:\n",
      "\n",
      "1. Fly ðŸ›«: There are several airlines that offer direct or connecting flights from Beijing Capital International Airport (PEK) to John F. Kennedy International Airport (JFK) or LaGuardia Airport (LGA) in New York.\n",
      "2. Train ðŸš‚: You can take a train from Beijing to New York, but it's a long journey that takes several days. The train route goes through Moscow and then to New York.\n",
      "3. Bus ðŸšŒ: Taking a bus from Beijing to New York is another option, but it's a long journey that takes around 24 hours.\n",
      "4. Drive ðŸš—: If you prefer to drive, you can take a car from Beijing to New York, but it's a long journey that takes around 20 days.\n",
      "\n",
      "Let me know if you have any other questions! ðŸ˜Š\n",
      "\n",
      "==================================\n",
      "\n",
      "CPU times: user 1.89 ms, sys: 3.54 ms, total: 5.43 ms\n",
      "Wall time: 6.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Always answer with emojis\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": \"How to go from Beijing to NY?\"},\n",
    "        ]\n",
    "    ],\n",
    "    \"parameters\": {\"max_new_tokens\": 512, \"top_p\": 0.9, \"temperature\": 0.6},\n",
    "}\n",
    "try:\n",
    "    response = predictor.predict(payload, custom_attributes=\"accept_eula=false\")\n",
    "    print_dialog(payload, response)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713b8143-b265-4bb7-8f23-94940dee5925",
   "metadata": {},
   "source": [
    "***\n",
    "### Code generation\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "06060f06-86c4-4fa1-8bd7-ac5aa10d3187",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Write a program to compute factorial in python:\n",
      "\n",
      "> Assistant:  Sure! Here is a simple program in Python to compute the factorial of a given number:\n",
      "```\n",
      "def factorial(n):\n",
      "    if n == 0:\n",
      "        return 1\n",
      "    else:\n",
      "        return n * factorial(n-1)\n",
      "```\n",
      "Explanation:\n",
      "\n",
      "* The function `factorial` takes in a single argument `n`, which is the number for which we want to compute the factorial.\n",
      "* We use a recursive approach to compute the factorial. If `n` is 0, the factorial is defined to be 1. Otherwise, we multiply `n` by the factorial of `n-1`.\n",
      "* The function returns the computed factorial.\n",
      "\n",
      "Here is an example of how to use the `factorial` function:\n",
      "```\n",
      "print(factorial(5))  # prints 120\n",
      "```\n",
      "I hope this helps! Let me know if you have any questions.\n",
      "\n",
      "==================================\n",
      "\n",
      "CPU times: user 6.79 ms, sys: 180 Âµs, total: 6.97 ms\n",
      "Wall time: 6.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": \"Write a program to compute factorial in python:\"},\n",
    "        ]\n",
    "    ],\n",
    "    \"parameters\": {\"max_new_tokens\": 512, \"top_p\": 0.9, \"temperature\": 0.6},\n",
    "}\n",
    "try:\n",
    "    response = predictor.predict(payload, custom_attributes=\"accept_eula=false\")\n",
    "    print_dialog(payload, response)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ca6e5e-6775-4ee2-a90d-d68452489097",
   "metadata": {},
   "source": [
    "***\n",
    "### Content Generation\n",
    "One shot prompting\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "89bf3406-749d-4e76-ac18-4b39839ae33a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Building a website can be done in 10 simple steps:\n",
      "\n",
      "> Assistant:  Sure! Here are 10 simple steps to help you build a website:\n",
      "\n",
      "Step 1: Define Your Website's Purpose and Goals\n",
      "Before you start building your website, you need to determine its purpose and goals. What do you want your website to achieve? What kind of content will you publish on it? Who is your target audience? Answering these questions will help you create a clear vision for your website and guide your decision-making throughout the building process.\n",
      "\n",
      "Step 2: Choose a Domain Name and Web Hosting Provider\n",
      "Your domain name is the address of your website (e.g., [www.yoursite.com](http://www.yoursite.com)), while your web hosting provider is the company that will store your website's files and make them accessible to the public. You can choose from a variety of domain name registrars and web hosting providers, but make sure to do your research and choose a reputable one to ensure your website is reliable and secure.\n",
      "\n",
      "Step 3: Plan Your Website's Structure and Navigation\n",
      "The structure and navigation of your website are crucial for user experience and search engine optimization (SEO). Determine the main sections of your website, how they will be organized, and how users will navigate between them. You can create a rough wireframe of your website to help you visualize the structure and make adjustments as needed.\n",
      "\n",
      "Step 4: Design Your Website's Layout and Visual Elements\n",
      "The layout and visual elements of your website should be visually appealing and easy to navigate. You can use a website builder or design software like Adobe Dreamweaver or Sketch to create your website's layout, choose a color scheme and fonts, and add images, videos, or other multimedia elements.\n",
      "\n",
      "Step 5: Write and Optimize Your Website's Content\n",
      "Your website's content should be engaging, informative, and optimized for search engines. Write high-quality, keyword-rich content for your website's pages, and make sure it is well-structured and easy to read. Use headings, subheadings, and bullet points to break up the content and make it more scannable.\n",
      "\n",
      "Step 6: Add Interactive Elements and Features\n",
      "To make your website more engaging and interactive, you can add features like contact forms, social media integration, and interactive elements like animations\n",
      "\n",
      "==================================\n",
      "\n",
      "CPU times: user 4.93 ms, sys: 0 ns, total: 4.93 ms\n",
      "Wall time: 15.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": \"Building a website can be done in 10 simple steps:\"},\n",
    "        ]\n",
    "    ],\n",
    "    \"parameters\": {\"max_new_tokens\": 512, \"top_p\": 0.9, \"temperature\": 0.6},\n",
    "}\n",
    "try:\n",
    "    response = predictor.predict(payload, custom_attributes=\"accept_eula=false\")\n",
    "    print_dialog(payload, response)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc36742c-a076-4cfb-b81d-b391e4068d90",
   "metadata": {},
   "source": [
    "***\n",
    "### Translation\n",
    "Few shot learning\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "98fbe458-ddd6-4e01-a403-1672db0f3bab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Translate English to French:\n",
      "\n",
      "                                            sea otter => loutre de mer\n",
      "\n",
      "                                            peppermint => menthe poivrÃ©e\n",
      "\n",
      "                                            plush girafe => girafe peluche\n",
      "\n",
      "                                            cheese =>\n",
      "\n",
      "> Assistant:  Sure! Here are the translations for the remaining words:\n",
      "\n",
      "* sea otter: loutre de mer (as you already provided)\n",
      "* peppermint: menthe poivrÃ©e (corrected spelling)\n",
      "* plush giraffe: girafe peluche (corrected spelling)\n",
      "* cheese: fromage (this is the standard French word for \"cheese\")\n",
      "\n",
      "==================================\n",
      "\n",
      "CPU times: user 5.04 ms, sys: 0 ns, total: 5.04 ms\n",
      "Wall time: 2.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": \"\"\"Translate English to French:\n",
    "\n",
    "                                            sea otter => loutre de mer\n",
    "\n",
    "                                            peppermint => menthe poivrÃ©e\n",
    "\n",
    "                                            plush girafe => girafe peluche\n",
    "\n",
    "                                            cheese =>\"\"\"\n",
    "            },\n",
    "        ]\n",
    "    ],\n",
    "    \"parameters\": {\"max_new_tokens\": 512, \"top_p\": 0.9, \"temperature\": 0.6},\n",
    "}\n",
    "try:\n",
    "    response = predictor.predict(payload, custom_attributes=\"accept_eula=false\")\n",
    "    print_dialog(payload, response)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55706ce0-731f-4e27-bb80-8d54853c7bca",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "### CoT Prompting\n",
    "[Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/pdf/2201.11903.pdf)  \n",
    "[Large Language Models are Zero-Shot Reasoners](https://arxiv.org/pdf/2205.11916.pdf)  \n",
    "\n",
    "Are incorporated in the more recent models such as Llama-2, and thus these mdoels are able to ahndle these prompts out of the box\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c86d1717-90c9-4e4b-a74e-77fda1f71604",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
      "\n",
      "> Assistant:  Great, let's solve this problem step by step!\n",
      "\n",
      "You started with 10 apples.\n",
      "\n",
      "* You gave 2 apples to the neighbor, so you have 10 - 2 = 8 apples left.\n",
      "* You gave 2 apples to the repairman, so you have 8 - 2 = 6 apples left.\n",
      "\n",
      "Then, you went and bought 5 more apples. So, you now have 6 + 5 = 11 apples.\n",
      "\n",
      "Finally, you ate 1 apple, so you have 11 - 1 = 10 apples left.\n",
      "\n",
      "Therefore, you remain with 10 apples.\n",
      "\n",
      "==================================\n",
      "\n",
      "CPU times: user 18.3 ms, sys: 270 Âµs, total: 18.6 ms\n",
      "Wall time: 5.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": \"\"\"I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\"\"\"\n",
    "            },\n",
    "        ]\n",
    "    ],\n",
    "    \"parameters\": {\"max_new_tokens\": 512, \"top_p\": 0.9, \"temperature\": 0.6},\n",
    "}\n",
    "try:\n",
    "    response = predictor.predict(payload, custom_attributes=\"accept_eula=false\")\n",
    "    print_dialog(payload, response)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb16cf2d-92ca-4ab8-96b7-d7756284c07f",
   "metadata": {},
   "source": [
    "***\n",
    "### Sentiment-analysis\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed841b20-d039-4369-b769-42ac9245166f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: \"I hate it when my phone battery dies.\"\n",
      "                                            Sentiment: Negative\n",
      "                                            ###\n",
      "                                            Tweet: \"My day has been :+1:\"\n",
      "                                            Sentiment: Positive\n",
      "                                            ###\n",
      "                                            Tweet: \"This is the link to the article\"\n",
      "                                            Sentiment: Neutral\n",
      "                                            ###\n",
      "                                            Tweet: \"This new music video was incredibile\"\n",
      "                                            Sentiment:\n",
      "\n",
      "> Assistant:  Sure! Here are the sentiment analyses for the given tweets:\n",
      "\n",
      "1. \"I hate it when my phone battery dies.\" - Negative\n",
      "2. \"My day has been :+1:\" - Positive\n",
      "3. \"This is the link to the article\" - Neutral\n",
      "4. \"This new music video was incredibile\" - Positive\n",
      "\n",
      "In the first tweet, the user expresses displeasure at their phone battery dying, which is a negative sentiment.\n",
      "\n",
      "In the second tweet, the user uses a positive emoji to indicate a good day, indicating a positive sentiment.\n",
      "\n",
      "In the third tweet, the user provides a neutral sentiment by simply sharing a link without any additional context or emotion.\n",
      "\n",
      "In the fourth tweet, the user uses a positive adjective to describe the music video, indicating a positive sentiment.\n",
      "\n",
      "==================================\n",
      "\n",
      "CPU times: user 2.66 ms, sys: 0 ns, total: 2.66 ms\n",
      "Wall time: 5.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": \"\"\"\"I hate it when my phone battery dies.\"\n",
    "                                            Sentiment: Negative\n",
    "                                            ###\n",
    "                                            Tweet: \"My day has been :+1:\"\n",
    "                                            Sentiment: Positive\n",
    "                                            ###\n",
    "                                            Tweet: \"This is the link to the article\"\n",
    "                                            Sentiment: Neutral\n",
    "                                            ###\n",
    "                                            Tweet: \"This new music video was incredibile\"\n",
    "                                            Sentiment:\"\"\"\n",
    "            },\n",
    "        ]\n",
    "    ],\n",
    "    \"parameters\": {\"max_new_tokens\": 512, \"top_p\": 0.9, \"temperature\": 0.6},\n",
    "}\n",
    "try:\n",
    "    response = predictor.predict(payload, custom_attributes=\"accept_eula=false\")\n",
    "    print_dialog(payload, response)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913a65de-c860-4e20-8de5-ec23a69527f3",
   "metadata": {},
   "source": [
    "***\n",
    "### Q n A\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5206793b-cda3-43dc-9c62-2055c0bfc2ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Could you remind me when was the C programming language invented?\n",
      "\n",
      "> Assistant:  The C programming language was invented in the early 1970s by Dennis Ritchie, a computer scientist at Bell Labs.\n",
      "\n",
      "The first edition of the C language specification, known as \"ANSI C,\" was published in 1983 by the American National Standards Institute (ANSI). This standardized the language and provided a common definition for C programming.\n",
      "\n",
      "So, the C programming language was invented in the early 1970s, and the first standard for the language was published in 1983.\n",
      "\n",
      "==================================\n",
      "\n",
      "CPU times: user 0 ns, sys: 4.94 ms, total: 4.94 ms\n",
      "Wall time: 3.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": \"Could you remind me when was the C programming language invented?\"\n",
    "            },\n",
    "        ]\n",
    "    ],\n",
    "    \"parameters\": {\"max_new_tokens\": 512, \"top_p\": 0.9, \"temperature\": 0.6},\n",
    "}\n",
    "try:\n",
    "    response = predictor.predict(payload, custom_attributes=\"accept_eula=false\")\n",
    "    print_dialog(payload, response)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea6a8f5-f783-41a7-8af6-8545643b635c",
   "metadata": {},
   "source": [
    "***\n",
    "### Recipe generation\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e279fcfe-2d38-4380-b021-dfe6b99eb47d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is the recipe for a delicious lemon rice?\n",
      "\n",
      "> Assistant:  Lemon rice is a popular side dish in many cuisines, particularly in Indian and Middle Eastern cooking. Here is a simple recipe for a delicious lemon rice dish:\n",
      "\n",
      "Ingredients:\n",
      "\n",
      "* 1 cup uncooked white or brown rice\n",
      "* 2 cups water\n",
      "* 1/2 cup freshly squeezed lemon juice (about 2-3 lemons)\n",
      "* 2 tablespoons vegetable oil\n",
      "* 1 small onion, finely chopped\n",
      "* 1 small carrot, grated\n",
      "* 1 teaspoon ground cumin\n",
      "* 1 teaspoon ground coriander\n",
      "* Salt and pepper, to taste\n",
      "* Fresh cilantro, chopped (optional)\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. Rinse the rice in a fine-mesh sieve until the water runs clear. Drain and set aside.\n",
      "2. Heat the oil in a large saucepan over medium heat. Add the onion and cook, stirring occasionally, until it is lightly browned and translucent, about 5 minutes.\n",
      "3. Add the grated carrot and cook for another 2-3 minutes, stirring occasionally.\n",
      "4. Add the rice to the saucepan and stir to coat the rice with the oil and mix with the onion and carrot.\n",
      "5. Pour in the water and lemon juice, and bring the mixture to a boil.\n",
      "6. Reduce the heat to low, cover the saucepan with a tight-fitting lid, and simmer for 18-20 minutes, or until the water has been absorbed and the rice is cooked.\n",
      "7. Fluff the rice with a fork and season with salt, pepper, cumin, and coriander.\n",
      "8. Garnish with chopped cilantro, if desired, and serve hot.\n",
      "\n",
      "Tips:\n",
      "\n",
      "* Use fresh and high-quality ingredients for the best flavor.\n",
      "* Adjust the amount of lemon juice to your taste. Some people prefer a more acidic taste, while others prefer a milder flavor.\n",
      "* You can also add other ingredients to the rice, such as diced vegetables (e.g., peas, corn), nuts,\n",
      "\n",
      "==================================\n",
      "\n",
      "CPU times: user 5.23 ms, sys: 370 Âµs, total: 5.6 ms\n",
      "Wall time: 15.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\":  \"What is the recipe for a delicious lemon rice?\"\n",
    "            },\n",
    "        ]\n",
    "    ],\n",
    "    \"parameters\": {\"max_new_tokens\": 512, \"top_p\": 0.9, \"temperature\": 0.6},\n",
    "}\n",
    "try:\n",
    "    response = predictor.predict(payload, custom_attributes=\"accept_eula=false\")\n",
    "    print_dialog(payload, response)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b09b57-9fbb-4d1c-bcf9-631ce0cb116c",
   "metadata": {},
   "source": [
    "  \n",
    "  \n",
    "***\n",
    "### Summarization\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa76f728-4599-4b19-8702-837cbe88e1b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Starting today, the state-of-the-art Falcon 40B foundation model from Technology\n",
      "    Innovation Institute (TII) is available on Amazon SageMaker JumpStart, SageMaker's machine learning (ML) hub\n",
      "    that offers pre-trained models, built-in algorithms, and pre-built solution templates to help you quickly get\n",
      "    started with ML. You can deploy and use this Falcon LLM with a few clicks in SageMaker Studio or\n",
      "    programmatically through the SageMaker Python SDK.\n",
      "    Falcon 40B is a 40-billion-parameter large language model (LLM) available under the Apache 2.0 license that\n",
      "    ranked #1 in Hugging Face Open LLM leaderboard, which tracks, ranks, and evaluates LLMs across multiple\n",
      "    benchmarks to identify top performing models. Since its release in May 2023, Falcon 40B has demonstrated\n",
      "    exceptional performance without specialized fine-tuning. To make it easier for customers to access this\n",
      "    state-of-the-art model, AWS has made Falcon 40B available to customers via Amazon SageMaker JumpStart.\n",
      "    Now customers can quickly and easily deploy their own Falcon 40B model and customize it to fit their specific\n",
      "    needs for applications such as translation, question answering, and summarizing information.\n",
      "    Falcon 40B are generally available today through Amazon SageMaker JumpStart in US East (Ohio),\n",
      "    US East (N. Virginia), US West (Oregon), Asia Pacific (Tokyo), Asia Pacific (Seoul), Asia Pacific (Mumbai),\n",
      "    Europe (London), Europe (Frankfurt), Europe (Ireland), and Canada (Central),\n",
      "    with availability in additional AWS Regions coming soon. To learn how to use this new feature,\n",
      "    please see SageMaker JumpStart documentation, the Introduction to SageMaker JumpStart â€“\n",
      "    Text Generation with Falcon LLMs example notebook, and the blog Technology Innovation Institute trainsthe\n",
      "    state-of-the-art Falcon LLM 40B foundation model on Amazon SageMaker. Summarize the article above:\n",
      "\n",
      "> Assistant:  The state-of-the-art Falcon 40B foundation model from Technology Innovation Institute (TII) is now available on Amazon SageMaker JumpStart, a machine learning (ML) hub that offers pre-trained models, built-in algorithms, and pre-built solution templates. With just a few clicks in SageMaker Studio or through the SageMaker Python SDK, customers can deploy and use this 40-billion-parameter large language model (LLM) for applications such as translation, question answering, and summarizing information. Available in multiple AWS Regions, Falcon 40B is generally available today through Amazon SageMaker JumpStart, with availability in additional Regions coming soon. To learn more, customers can refer to the SageMaker JumpStart documentation, the Introduction to SageMaker JumpStart â€“ Text Generation with Falcon LLMs example notebook, and the TII blog post.\n",
      "\n",
      "==================================\n",
      "\n",
      "CPU times: user 4.92 ms, sys: 0 ns, total: 4.92 ms\n",
      "Wall time: 6.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\":  \"\"\"Starting today, the state-of-the-art Falcon 40B foundation model from Technology\n",
    "    Innovation Institute (TII) is available on Amazon SageMaker JumpStart, SageMaker's machine learning (ML) hub\n",
    "    that offers pre-trained models, built-in algorithms, and pre-built solution templates to help you quickly get\n",
    "    started with ML. You can deploy and use this Falcon LLM with a few clicks in SageMaker Studio or\n",
    "    programmatically through the SageMaker Python SDK.\n",
    "    Falcon 40B is a 40-billion-parameter large language model (LLM) available under the Apache 2.0 license that\n",
    "    ranked #1 in Hugging Face Open LLM leaderboard, which tracks, ranks, and evaluates LLMs across multiple\n",
    "    benchmarks to identify top performing models. Since its release in May 2023, Falcon 40B has demonstrated\n",
    "    exceptional performance without specialized fine-tuning. To make it easier for customers to access this\n",
    "    state-of-the-art model, AWS has made Falcon 40B available to customers via Amazon SageMaker JumpStart.\n",
    "    Now customers can quickly and easily deploy their own Falcon 40B model and customize it to fit their specific\n",
    "    needs for applications such as translation, question answering, and summarizing information.\n",
    "    Falcon 40B are generally available today through Amazon SageMaker JumpStart in US East (Ohio),\n",
    "    US East (N. Virginia), US West (Oregon), Asia Pacific (Tokyo), Asia Pacific (Seoul), Asia Pacific (Mumbai),\n",
    "    Europe (London), Europe (Frankfurt), Europe (Ireland), and Canada (Central),\n",
    "    with availability in additional AWS Regions coming soon. To learn how to use this new feature,\n",
    "    please see SageMaker JumpStart documentation, the Introduction to SageMaker JumpStart â€“\n",
    "    Text Generation with Falcon LLMs example notebook, and the blog Technology Innovation Institute trainsthe\n",
    "    state-of-the-art Falcon LLM 40B foundation model on Amazon SageMaker. Summarize the article above:\"\"\"\n",
    "            },\n",
    "        ]\n",
    "    ],\n",
    "    \"parameters\": {\"max_new_tokens\": 512, \"top_p\": 0.9, \"temperature\": 0.6},\n",
    "}\n",
    "try:\n",
    "    response = predictor.predict(payload, custom_attributes=\"accept_eula=false\")\n",
    "    print_dialog(payload, response)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68238c99-2c79-4525-b123-5f701fc43634",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f1b18a-f5b6-4326-877d-cbada7d23a13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc54a581-4864-4335-8d4f-ba6c021e4f90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b25521d-f757-4145-af1d-9456e21c8006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:10:40.183282\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "end = time.time()\n",
    "print(timedelta(seconds=end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e062d29",
   "metadata": {},
   "source": [
    "## Clean up the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "24cc5560",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete the SageMaker endpoint\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
